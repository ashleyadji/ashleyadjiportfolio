{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EmailSpamClassifierStarter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEvUqxeqzVN6"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf6PxiZxQafL"
      },
      "source": [
        "filepath = \"https://static.junilearning.com/ai_level_2/emails.csv\"\n",
        "\n",
        "# Dataset: https://www.kaggle.com/venky73/spam-mails-dataset\n",
        "# read in data and remove unnecessary columns\n",
        "data = pd.read_csv(filepath)\n",
        "data = data.drop('messageid', axis = 1)\n",
        "data = data.drop('label', axis = 1)\n",
        "data.drop_duplicates(inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is NLTK?***\n",
        "REAL AI comes in! A living platform for python programs to work with human language data. does natural language processing, looks at language and makes it more usable by the computer\n",
        "\n",
        "classify emails as spam or not spam"
      ],
      "metadata": {
        "id": "8JaGc5jHmc7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write function that removes all punctuation and stopwords from an email\n",
        "#stopwords are unimportant words that don't give substance to content, removing these words avoids placing significance to those words\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "#HOW TO TOKENIZE\n",
        "  #loop through the string, check if each character is a punctuation mark by checking if it is in string.punctuation\n",
        "  # if it is not punctuation, append it to nopunctuation\n",
        "  # create list called no_stopwords\n",
        "  # for each word in nopunctuation, check if the word is in stopwords.words('english')\n",
        "  # if it is not, add it to no_stopwords\n",
        "  # convert to lowercase: <string>.lower()\n",
        "  #what is tokenizing? to break down text into smallest parts (single words), gets rid of punctuation, stopwords\n",
        "  #return a list of strings, tokens\n",
        "def tokenize(text):\n",
        "  nopunctuation = []\n",
        "  for x in text:\n",
        "    if x not in string.punctuation:\n",
        "      nopunctuation.append(x)\n",
        "  nopunctuationstr = ''.join(nopunctuation)\n",
        "  no_stopwords = []\n",
        "  for word in nopunctuationstr.split():\n",
        "    if word.lower() not in stopwords.words('english'):\n",
        "      no_stopwords.append(word)\n",
        "  return no_stopwords\n",
        "\n",
        "#build feature vectors through count words\n",
        "#use tokenize as analyzer, .fit_transform (text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kusFmVoCmV6k",
        "outputId": "b6f322b5-b465-44e3-e6c2-4ca127a998b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = CountVectorizer(analyzer=tokenize).fit_transform(data[\"text\"])\n",
        "y = data[\"label_num\"]"
      ],
      "metadata": {
        "id": "t7k9fuPGk9BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0])\n",
        "# TESTING FEATURE VECTOR: 0 is what email, 4026 is number representing word, 1 is count\n",
        "email = tokenize(data[\"text\"][0])\n",
        "\n",
        "unique = {}\n",
        "for i in email:\n",
        "  if i not in unique:\n",
        "    unique[i] = 1\n",
        "  else:\n",
        "    unique[i] += 1\n",
        "print(unique)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_ubcIeQmyOz",
        "outputId": "b8872d83-4338-4f2d-919b-f0530a52a953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 4026)\t1\n",
            "  (0, 15806)\t1\n",
            "  (0, 25870)\t1\n",
            "  (0, 25864)\t1\n",
            "  (0, 3963)\t1\n",
            "  (0, 17567)\t1\n",
            "  (0, 27632)\t1\n",
            "  (0, 18297)\t1\n",
            "  (0, 26428)\t1\n",
            "  (0, 1886)\t1\n",
            "  (0, 1442)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 30474)\t1\n",
            "  (0, 17477)\t1\n",
            "  (0, 12935)\t1\n",
            "  (0, 30933)\t1\n",
            "  (0, 12884)\t1\n",
            "  (0, 29956)\t1\n",
            "  (0, 28667)\t1\n",
            "  (0, 30170)\t1\n",
            "  (0, 12800)\t2\n",
            "  (0, 39652)\t1\n",
            "  (0, 30532)\t1\n",
            "  (0, 41511)\t1\n",
            "  (0, 32098)\t1\n",
            "  (0, 4397)\t1\n",
            "  (0, 27909)\t1\n",
            "  (0, 18251)\t1\n",
            "  (0, 11790)\t1\n",
            "  (0, 10063)\t1\n",
            "  (0, 27143)\t1\n",
            "  (0, 6158)\t1\n",
            "  (0, 15125)\t1\n",
            "  (0, 31126)\t1\n",
            "{'Subject': 1, 'enron': 1, 'methanol': 1, 'meter': 1, '988291': 1, 'follow': 1, 'note': 1, 'gave': 1, 'monday': 1, '4': 1, '3': 1, '00': 1, 'preliminary': 1, 'flow': 1, 'data': 1, 'provided': 1, 'daren': 1, 'please': 1, 'override': 1, 'pop': 1, 'daily': 2, 'volume': 1, 'presently': 1, 'zero': 1, 'reflect': 1, 'activity': 1, 'obtain': 1, 'gas': 1, 'control': 1, 'change': 1, 'needed': 1, 'asap': 1, 'economics': 1, 'purposes': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split into test and train data\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25, random_state = 42)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBybsip6is7v",
        "outputId": "8e7a0eac-ffab-480c-ecff-755d92e8b7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYJ_FgwLlTSE",
        "outputId": "779a8d84-bcf6-449d-941a-56beb5cc6448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9761634506242906\n"
          ]
        }
      ]
    }
  ]
}